{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66619dc6",
   "metadata": {},
   "source": [
    "**Official implementation of paper \"Revisiting In-context Learning Inference Circuit in Large Language Models\" (ICLR 2025)**:\n",
    "\n",
    "### Ablation Study\n",
    "\n",
    "This experiment is to ablate the proposed ICL inference circuit and observe the performance changes, results are in Table 1, 2.\n",
    "\n",
    "Author: Hakaze Cho, yfzhao@jaist.ac.jp, 2024/09\n",
    "\n",
    "Organized, commented, and modified by: Hakaze Cho, 2025/02/03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b4ef9",
   "metadata": {},
   "source": [
    "**Part I: Import, Define, and Load Everything**\n",
    "\n",
    "What you should do:\n",
    "1. [Cell 1] Change to the path from your working directory to the directory containing the README.md file.\n",
    "2. [Cell 2] Define your experiment parameters.\n",
    "3. Run the Cell 1 - 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f3ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and change the working directory.\n",
    "\n",
    "## Change the working directory\n",
    "import os\n",
    "try:\n",
    "    # Change to the path from your working directory to the directory containing the README.md file.\n",
    "    os.chdir(\"ICL_Inference_Dynamics_Released\") \n",
    "except:\n",
    "    print(\"Already in the correct directory or the directory does not exist.\")\n",
    "\n",
    "## Import libraries\n",
    "from util import load_model_and_data, ablation_study\n",
    "import StaICC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95828fea",
   "metadata": {},
   "source": [
    "About the parameter `ablation_type`:\n",
    "\n",
    "Alternative: \n",
    "\n",
    "- `\"default_causal_attention_mask\"`: no ablation (line 1 and 6 (given `k = 0`) in Table 1) \n",
    "- `\"xi_to_si\"`: disconnect the attention link from every token in the demonstrations to its forerunner token (line 2 in Table 1)\n",
    "- `\"xq_to_sq\"`: disconnect the attention link from every token in the query to its forerunner token (line 3 in Table 1)\n",
    "- `\"si_to_yi\"`: disconnect the attention link from every forerunner token to its label token (line 4 in Table 1)\n",
    "- `\"yi_to_sq\"`: disconnect the attention link from every label token to the query's forerunner token (line 5 in Table 1)\n",
    "- `\"si_to_si\"`: disconnect the attention link from every forerunner token to subsequent forerunner token (Table 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992b12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model and huggingfacetoken configurations\n",
    "\n",
    "## The huggingface model name to be tested as the LM for ICL. \n",
    "## Recommended: \"meta-llama/Meta-Llama-3-8B\", \"tiiuae/falcon-7b\", \"meta-llama/Meta-Llama-3-70B\", \"tiiuae/falcon-40b\"\n",
    "ICL_model_name = \"tiiuae/falcon-7b\" \n",
    "\n",
    "## Whether to use the quantized version of the model. \n",
    "## This experiment is only available for unquantized models.\n",
    "quantized = False\n",
    "\n",
    "## The huggingface token to access the model. If you use the Llama model, you need to set this.\n",
    "huggingface_token = \"your token here\"\n",
    "\n",
    "\n",
    "# Experiment parameters\n",
    "\n",
    "## The selected token type to calculate the KA. Alternative: \"none\" (forerunner token s), \"label_words\" (y), \"last_sentence_token\" (x).\n",
    "ICL_selected_token_type = \"label_words\" \n",
    "\n",
    "## The demonstration numbers. Recommended: 0, 1, 2, 4, 8, 12.\n",
    "k = 4 \n",
    "\n",
    "## The used dataset index from the StaICC library. Alternative: 0, 1, 2, 3, 4, 5. See the README.md for more information.\n",
    "dataset_index = 2 \n",
    "\n",
    "## Force the ICL_model to reload, even the ICL_model is already in the variables. \n",
    "## Recommended: False.\n",
    "model_forced_reload = False\n",
    "\n",
    "## In which layers (ratios) the ablation is conducted? Start and end layer number percentages.\n",
    "## Same as the column label in the Table 1.\n",
    "maskstart = 0\n",
    "maskend = 0.25\n",
    "\n",
    "## Ablation type \n",
    "ablation_type = \"xq_to_sq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec570a8-43a2-43cd-8d0f-858fb9504d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Load the data and build the test inputs.\n",
    "\n",
    "bench = StaICC.Normal(k)\n",
    "prompts, queries = load_model_and_data.load_data_from_StaICC_experimentor(bench[dataset_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d973fd-9b51-464a-a356-7474bdff9c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load the models.\n",
    "\n",
    "vars_dict = vars() if \"ICL_model\" in vars() else locals()\n",
    "if \"ICL_model\" not in vars_dict or model_forced_reload:\n",
    "    ICL_model, ICL_tknz = load_model_and_data.load_ICL_model(ICL_model_name, huggingface_token = huggingface_token, quantized = quantized)\n",
    "    loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec708a95",
   "metadata": {},
   "source": [
    "**Part II: Run the Experiment**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe29af8-91ca-4a56-94b0-700d9e0fe088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Run the ablation study.\n",
    "\n",
    "## Replace the inference with our method to enable the ablation study.\n",
    "if \"falcon\" in ICL_model_name:\n",
    "    inference_type = \"falcon\"\n",
    "    ablation_study.make_falcon_model(ICL_model)\n",
    "else:\n",
    "    inference_type = \"transformer\"\n",
    "    ablation_study.make_llama_model(ICL_model)\n",
    "\n",
    "## Run the inference.\n",
    "exp, ctrl = ablation_study.Masked_ICL_inference(\n",
    "    prompts,\n",
    "    ICL_model,\n",
    "    ICL_tknz, \n",
    "    bench[dataset_index],\n",
    "    inference_type = inference_type,\n",
    "    mask_start_layer = int(maskstart * ICL_model.config.num_hidden_layers),\n",
    "    mask_end_layer = int(maskend * ICL_model.config.num_hidden_layers),\n",
    "    attention_mask_type = ablation_type,\n",
    "    run_control_experiment_parallelly = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bdaf6",
   "metadata": {},
   "source": [
    "**Part III: Report the Result**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 5, and read the results in the echo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5839d-28b1-48e1-b35f-b77ac60f3d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Calculate the accuracy.\n",
    "\n",
    "print(\"Accuracy of the Ablation Study on dataset: \" + str(dataset_index) + \", Model: \" + ICL_model_name + \", Ablation Type: \" + ablation_type + \", Mask Start: \" + str(maskstart) + \", Mask End: \" + str(maskend))\n",
    "bench[dataset_index]._repeat_times = 1\n",
    "print(bench[dataset_index](input_prediction = exp)[0]['accuracy'])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Accuracy of the CONTROLLED Ablation Study on dataset: \" + str(dataset_index) + \", Model: \" + ICL_model_name + \", Ablation Type: Random\" + \", Mask Start: \" + str(maskstart) + \", Mask End: \" + str(maskend))\n",
    "bench[dataset_index]._repeat_times = 1\n",
    "print(bench[dataset_index](input_prediction = ctrl)[0]['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
