{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a577eb89",
   "metadata": {},
   "source": [
    "**Official implementation of paper \"Revisiting In-context Learning Inference Circuit in Large Language Models\" (ICLR 2025)**:\n",
    "\n",
    "### Induction Head Counting\n",
    "\n",
    "This experiment is to count the number of induction heads and correct induction heads in each layer, also calculate the accuracy based on various induction. Results in Fig. 6 (Left, Middle).\n",
    "\n",
    "Author: Hakaze Cho, yfzhao@jaist.ac.jp, 2024/08\n",
    "\n",
    "Organized, commented, and modified by: Hakaze Cho, 2025/01/29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029d628",
   "metadata": {},
   "source": [
    "**Part I: Import, Define, and Load Everything**\n",
    "\n",
    "What you should do:\n",
    "1. [Cell 1] Change to the path from your working directory to the directory containing the README.md file.\n",
    "2. [Cell 2] Define your experiment parameters.\n",
    "3. Run the Cell 1 and Cell 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and change the working directory.\n",
    "\n",
    "## Change the working directory\n",
    "import os\n",
    "try:\n",
    "    # Change to the path from your working directory to the directory containing the README.md file.\n",
    "    os.chdir(\"ICL_Inference_Dynamics_Released\") \n",
    "except:\n",
    "    print(\"Already in the correct directory or the directory does not exist.\")\n",
    "\n",
    "## Import libraries\n",
    "from util import load_model_and_data, inference, induction_evaluation\n",
    "import StaICC\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "## Some definations for the plots.\n",
    "plt.style.use('default')\n",
    "plt.rc('font',family='Cambria Math')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Cambria Math'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d301bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model and huggingfacetoken configurations\n",
    "\n",
    "## The huggingface model name to be tested as the LM for ICL. \n",
    "## Recommended: \"meta-llama/Meta-Llama-3-8B\", \"tiiuae/falcon-7b\", \"meta-llama/Meta-Llama-3-70B\", \"tiiuae/falcon-40b\"\n",
    "ICL_model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "## Whether to use the quantized version of the model. \n",
    "## Recommended: Keep it default.\n",
    "quantized = False if ICL_model_name in [\"meta-llama/Meta-Llama-3-8B\", \"EleutherAI/pythia-6.9b\", \"tiiuae/falcon-7b\"] else True\n",
    "\n",
    "## The huggingface token to access the model. If you use the Llama model, you need to set this.\n",
    "huggingface_token = \"your token here\"\n",
    "\n",
    "\n",
    "# Experiment parameters\n",
    "\n",
    "## The demonstration numbers. Recommended: 0, 1, 2, 4, 8, 12.\n",
    "k = 4 \n",
    "\n",
    "## The used dataset index from the StaICC library. Alternative: 0, 1, 2, 3, 4, 5. See the README.md for more information.\n",
    "dataset_index = 2 \n",
    "\n",
    "## Force the ICL_model to reload, even the ICL_model is already in the variables. \n",
    "## Recommended: False.\n",
    "model_forced_reload = False\n",
    "\n",
    "## Force the experiment to be redone, even the intermediate results are already in the path `experiment_material`.\n",
    "## Recommended: False.\n",
    "experiment_forced_redo = False\n",
    "\n",
    "## Define the const in the threshold to be judged as a forerunner token head. (e.g., \"5\" in 5/n_t)\n",
    "induction_head_threthold_times = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57a11d-912c-485e-a9a6-8f3c0ebb0634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Load the data and build the test inputs.\n",
    "\n",
    "bench = StaICC.Normal(k)\n",
    "prompts, queries = load_model_and_data.load_data_from_StaICC_experimentor(bench[dataset_index], \"label_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f0d50-8688-4e0e-94e2-81a9ae6d10d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load the model. \n",
    "\n",
    "vars_dict = vars() if \"ICL_model\" in vars() else locals()\n",
    "if \"ICL_model\" not in vars_dict or model_forced_reload:\n",
    "    ICL_model, ICL_tknz = load_model_and_data.load_ICL_model(ICL_model_name, huggingface_token = huggingface_token, quantized = quantized)\n",
    "    loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc97479",
   "metadata": {},
   "source": [
    "**Part II: Run the Experiment**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 5 - 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8062b40-088c-4b80-940b-60ffe26c1ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Inference the hidden states and save the intermediate results. If the intermediate results in path `experiment_matrial` (shown below) has been detected, automatically load the results.\n",
    "\n",
    "data_file_name = \"experiment_matrial/\" + ICL_model_name.replace('/', '_') + \",induc_Hidd_att\" + ',' + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "if os.path.exists(data_file_name):\n",
    "    with open(data_file_name, 'rb') as f:\n",
    "        ICL_hidden_states = pickle.load(f)\n",
    "        print(\"loaded\")\n",
    "else:\n",
    "    ICL_hidden_states = inference.step3_get_fl_feature_and_lastftol_attention(ICL_model, ICL_tknz, prompts, bench[dataset_index])\n",
    "    with open(data_file_name, 'wb') as f:\n",
    "        pickle.dump(ICL_hidden_states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802a8776-0eba-470c-b154-43bfd22eea71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Calculate the counted times as induction head for each attention head, and calculate the layered statistics.\n",
    "\n",
    "mean_max_magnitude = []\n",
    "mean_head_count = []\n",
    "\n",
    "for layers in range(len(ICL_hidden_states[1][0])):\n",
    "    temp = []\n",
    "    head_count = []\n",
    "    for sample in range(len(ICL_hidden_states[1])):\n",
    "        thre = induction_evaluation.get_theresold_magnitude_from_prompt(ICL_tknz, prompts[sample], induction_head_threthold_times, 1)\n",
    "        magnitudes = induction_evaluation.get_induction_magnitude_for_single_layer(ICL_hidden_states[1], bench[dataset_index], sample, layers)\n",
    "        temp.append(max(magnitudes))\n",
    "        count = 0\n",
    "        for temp_res in magnitudes:\n",
    "            if temp_res > thre:\n",
    "                count += 1\n",
    "        head_count.append(count)\n",
    "    mean_max_magnitude.append(np.mean(temp))\n",
    "    mean_head_count.append(np.mean(head_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aaf328d-f2e3-4eb9-a8a4-9b076b0836e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7: Calculate the counted times as **correct** induction head for each attention head, and calculate the layered statistics.\n",
    "\n",
    "mean_max_correct_magnitude = []\n",
    "mean_correct_head_count = []\n",
    "correct_induction_head_statics = []\n",
    "\n",
    "for layers in range(len(ICL_hidden_states[1][0])):\n",
    "    temp = []\n",
    "    head_count = []\n",
    "    layer_head_statics = [0] * len(ICL_hidden_states[1][0][0])\n",
    "    for sample in range(len(ICL_hidden_states[1])):\n",
    "        thre = induction_evaluation.get_theresold_correctness_from_prompt(ICL_tknz, prompts[sample], induction_head_threthold_times, len(bench[dataset_index].get_label_space()), 1)\n",
    "        magnitudes = induction_evaluation.get_induction_correctness_for_single_layer(ICL_hidden_states[1], bench[dataset_index], sample, layers)\n",
    "        temp.append(max(magnitudes))\n",
    "        count = 0\n",
    "        for i, temp_res in enumerate(magnitudes):\n",
    "            if temp_res > thre:\n",
    "                count += 1\n",
    "                layer_head_statics[i] += 1\n",
    "        head_count.append(count)\n",
    "    correct_induction_head_statics.append(layer_head_statics)\n",
    "    mean_max_correct_magnitude.append(np.mean(temp))\n",
    "    mean_correct_head_count.append(np.mean(head_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd863501",
   "metadata": {},
   "source": [
    "**Part III: Plot and Save the Result**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 8 - 10. You can define your own file name and dictionary to save the result in Cell 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Plot the counted times for each attention head as a correct head in a heatmap.\n",
    "\n",
    "r = plt.imshow(correct_induction_head_statics, cmap = 'Greens', vmin = 0)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar(r, shrink=0.5)\n",
    "plt.xlabel('Head #', fontsize = 12)\n",
    "plt.ylabel('Transformer Block', fontsize = 12) \n",
    "plt.title(\"Correct Induction Head Counted\" + \"\\n Dataset \" + str(dataset_index + 1) + \" with k = \" + str(k) + \"\\n model: \" + ICL_model_name, fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Plot the figure like Fig. 6 (Left).\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(range(1,len(mean_head_count) + 1), mean_head_count, color = \"#257ab6\", label = \"Induction\")\n",
    "plt.plot(range(1,len(mean_correct_head_count) + 1), mean_correct_head_count, color = \"green\", label = \"Correct Induction\")\n",
    "plt.xlim(-1, len(mean_correct_head_count) + 1)\n",
    "plt.xlabel(\"Transformer Block Number\", fontsize = 12)\n",
    "plt.ylabel(\"Induction Head #\", fontsize = 12)\n",
    "plt.title(\"Induction Head Counted\" + \"\\n Dataset \" + str(dataset_index + 1) + \" with k = \" + str(k) + \"\\n model: \" + ICL_model_name, fontsize = 12)\n",
    "\n",
    "ax = plt.gca()\n",
    "ylim = ax.get_ylim()\n",
    "plt.ylim(ylim)\n",
    "\n",
    "xrange = ax.get_xticks()\n",
    "xrange[1] = 1\n",
    "plt.xticks(xrange[1:-1])\n",
    "\n",
    "plt.legend(loc = 1, prop={'size': 9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ba7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save the induction head and correct induction head results.\n",
    "# Result file organization:\n",
    "# (mean_max_magnitude: list[layer_index] = max induction attention, mean_head_count: list[layer_index] = induction head count)\n",
    "# (mean_max_correct_magnitude: list[layer_index] = max correct induction attention, mean_correct_head_count: list[layer_index] = induction induction head count)\n",
    "\n",
    "data_file_name = \"data/\" + ICL_model_name.replace('/', '_') + \",induction_magnitude\" + ',' + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "with open(data_file_name, 'wb') as f:\n",
    "    pickle.dump([mean_max_magnitude, mean_head_count], f)\n",
    "\n",
    "data_file_name = \"data/\" + ICL_model_name.replace('/', '_') + \",correct_induction_magnitude\" + ',' + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "with open(data_file_name, 'wb') as f:\n",
    "    pickle.dump([mean_max_correct_magnitude, mean_correct_head_count], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd034ad",
   "metadata": {},
   "source": [
    "**Part IV: Calculate the data for Fig. 6 (Middle), and plot / save**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 11 - 13. You can define your own file name and dictionary to save the result in Cell 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d87df-8ea3-4d24-9798-333f1c0450aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 11: Calculate the mean accuracy based on the full space similarity.\n",
    "\n",
    "mean_accuracy_based_on_full_space_similarity = []\n",
    "mean_accuracy_based_on_besthead = []\n",
    "mean_accuracy_based_on_averagehead = []\n",
    "\n",
    "for layers in tqdm(range(len(ICL_hidden_states[1][0]))):\n",
    "    temp = []\n",
    "    for sample in range(len(ICL_hidden_states[1])):\n",
    "        temp.append(induction_evaluation.get_induction_likelihood_full_space_similarity(ICL_hidden_states[0], bench[dataset_index], sample, layers))\n",
    "    mean_accuracy_based_on_full_space_similarity.append(np.mean(temp))\n",
    "\n",
    "for layers in tqdm(range(len(ICL_hidden_states[1][0]))):\n",
    "    temp = []\n",
    "    averaged_temp = []\n",
    "    for sample in range(len(ICL_hidden_states[1])):\n",
    "        temp.append(np.max(induction_evaluation.get_induction_likelihood_head(ICL_hidden_states[1], bench[dataset_index], sample, layers)))\n",
    "        averaged_temp.append(np.mean(induction_evaluation.get_induction_likelihood_head(ICL_hidden_states[1], bench[dataset_index], sample, layers)))\n",
    "    mean_accuracy_based_on_besthead.append(np.mean(temp))\n",
    "    mean_accuracy_based_on_averagehead.append(np.mean(averaged_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Plot the figure like Fig. 6 (Middle).\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(range(1,len(mean_accuracy_based_on_full_space_similarity) + 1), mean_accuracy_based_on_full_space_similarity, color = \"#ff7f0e\", label = \"Vanilla Attention\")\n",
    "plt.plot(range(1,len(mean_accuracy_based_on_full_space_similarity) + 1), mean_accuracy_based_on_besthead, color = \"green\", label = \"Best Ind. Head\")\n",
    "plt.plot(range(1,len(mean_accuracy_based_on_full_space_similarity) + 1), mean_accuracy_based_on_averagehead, color = \"#257ab6\", label = \"Head Average\")\n",
    "plt.xlabel(\"Transformer Block Number\", fontsize = 12)\n",
    "plt.ylabel(\"Correct Label Assignment\", fontsize = 12)\n",
    "plt.axhline(1/len(bench[dataset_index].get_label_space()), color = \"black\", linestyle = \"--\", linewidth = 1, label = \"Random\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ylim = ax.get_ylim()\n",
    "plt.ylim((0.17, ylim[1]))\n",
    "xrange = ax.get_xticks()\n",
    "xrange[1] = 1\n",
    "plt.xticks(xrange[1:-1])\n",
    "plt.xlim(0, len(mean_accuracy_based_on_full_space_similarity) + 1)\n",
    "plt.legend(loc = 4, prop={'size': 9}, ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de06a311-5093-4221-b10a-9377c1a306b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 13: Save the accuracy results in different assignments.\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_file_name = \"data/\" + ICL_model_name.replace('/', '_')+ \",induction_likelihood\" + ',' + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "with open(data_file_name, 'wb') as f:\n",
    "    pickle.dump([mean_accuracy_based_on_full_space_similarity, mean_accuracy_based_on_besthead, mean_accuracy_based_on_averagehead], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
