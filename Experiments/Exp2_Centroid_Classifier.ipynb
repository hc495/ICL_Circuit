{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e2cd9a",
   "metadata": {},
   "source": [
    "**Official implementation of paper \"Revisiting In-context Learning Inference Circuit in Large Language Models\" (ICLR 2025)**:\n",
    "\n",
    "### The Centroid Classifier Probing\n",
    "\n",
    "This experiment is to train centroid classifiers on the ICL hidden states, then test the accuracies to get whether the information in the hidden states is sufficient for ICL task (as shown in Fig. 3). Also, by controlling the selection of different hidden states, we can conduct a control experiment as shown in Fig. 5 (Right).\n",
    "\n",
    "Author: Hakaze Cho, yfzhao@jaist.ac.jp, 2024/09\n",
    "\n",
    "Organized, commented, and modified by: Hakaze Cho, 2025/01/26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a78dbc",
   "metadata": {},
   "source": [
    "**Part I: Import, Define, and Load Everything**\n",
    "\n",
    "What you should do:\n",
    "1. [Cell 1] Change to the path from your working directory to the directory containing the README.md file.\n",
    "2. [Cell 2] Define your experiment parameters.\n",
    "3. Run the Cell 1 and Cell 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and change the working directory.\n",
    "\n",
    "## Change the working directory\n",
    "import os\n",
    "try:\n",
    "    # Change to the path from your working directory to the directory containing the README.md file.\n",
    "    os.chdir(\"ICL_Inference_Dynamics_Released\") \n",
    "except:\n",
    "    print(\"Already in the correct directory or the directory does not exist.\")\n",
    "\n",
    "## Import libraries\n",
    "from util import load_model_and_data, inference\n",
    "import StaICC\n",
    "import matplotlib.pyplot as plt\n",
    "from util import layered_hidden_calibration\n",
    "import functools\n",
    "\n",
    "## Some definations for the plots.\n",
    "plt.style.use('default')\n",
    "plt.rc('font',family='Cambria Math')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Cambria Math'] + plt.rcParams['font.serif']\n",
    "\n",
    "EXPERIMENT_PRESETS = {\n",
    "    \"Fig3_Orange\": (\"none\", \"none\", True, False),\n",
    "    \"Fig3_Blue\": (\"last_sentence_token\", \"last_sentence_token\", True, False),\n",
    "    \"Fig5_Orange\": (\"none\", \"none\", True, False),\n",
    "    \"Fig5_Green_Solid\": (\"none\", \"label_words\", True, False),\n",
    "    \"Fig5_Red_Solid\": (\"none\", \"label_words\", False, False),\n",
    "    \"Fig5_Green_Dotted\": (\"label_words\", \"label_words\", True, False),\n",
    "    \"Fig5_Red_Dotted\": (\"label_words\", \"label_words\", False, False),\n",
    "    \"Fig5_Gray\": (\"none\", \"label_words\", True, True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe49654",
   "metadata": {},
   "source": [
    "**instructions on parameter `trained_token_type`, `predicted_token_type`, `corr_label`, and `no_context_baseline`:**\n",
    "\n",
    "- For Fig. 3:\n",
    "    - Orange: `trained_token_type = \"none\"`, `predicted_token_type = \"none\"`, `corr_label = True`, `no_context_baseline = False`\n",
    "    - Green: `trained_token_type = \"last_sentence_token\"`, `predicted_token_type = \"last_sentence_token\"`, `corr_label = True`, `no_context_baseline = False`\n",
    "- For Fig. 5 (Right): \n",
    "    - Solid curves: `trained_token_type = \"none\"`; Dotted curves: `trained_token_type = \"label_words\"`\n",
    "    - Green curves: `predicted_token_type = \"label_words\"`, `corr_label = True`, `no_context_baseline = False`\n",
    "    - Red curves: `predicted_token_type = \"label_words\"`, `corr_label = False`, `no_context_baseline = False`\n",
    "    - Orange curve: `predicted_token_type = \"none\"`, `corr_label = True`, `no_context_baseline = False`\n",
    "    - Gray curve: `predicted_token_type = \"label_words\"`, `corr_label = True`, `no_context_baseline = True`\n",
    "\n",
    "You can directly set the `experiment_presets` to the desired preset, defined in Cell 1, or set the parameters manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c4e7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model and huggingfacetoken configurations\n",
    "\n",
    "## The huggingface model name to be tested as the LM for ICL. \n",
    "## Recommended: \"meta-llama/Meta-Llama-3-8B\", \"EleutherAI/pythia-6.9b\", \"tiiuae/falcon-7b\", \"meta-llama/Meta-Llama-3-70B\", \"tiiuae/falcon-40b\"\n",
    "ICL_model_name = \"tiiuae/falcon-7b\" \n",
    "\n",
    "## Whether to use the quantized version of the model. \n",
    "## Recommended: Keep it default.\n",
    "quantized = False if ICL_model_name in [\"meta-llama/Meta-Llama-3-8B\", \"EleutherAI/pythia-6.9b\", \"tiiuae/falcon-7b\"] else True\n",
    "\n",
    "## The huggingface token to access the model. If you use the Llama model, you need to set this.\n",
    "huggingface_token = \"your token here\"\n",
    "\n",
    "\n",
    "# Experiment parameters\n",
    "\n",
    "## On which token the centroid classifier is trained. Alternative: \"none\" (forerunner token s), \"label_words\" (y), \"last_sentence_token\" (x).\n",
    "trained_token_type = \"label_words\"\n",
    "\n",
    "## On which token the centroid classifier predicts. Alternative: \"none\" (forerunner token s), \"label_words\" (y), \"last_sentence_token\" (x).\n",
    "predicted_token_type = \"label_words\" \n",
    "\n",
    "## Whether the **last** label in the prompt is (True) correct label or (False) wrong label.\n",
    "## Only effective when the `_token_type` is \"label_words\".\n",
    "corr_label = True\n",
    "\n",
    "## Correct Label w/o Context. That is, use classifier trained on the normal prompt to predict the label-token-only string.\n",
    "## Should only be true when you want to get the gray line in the Fig. 5 (Right).\n",
    "no_context_baseline = False\n",
    "\n",
    "## Experiment Presets\n",
    "## See cell 1 for the details. If not None, the above parameters will be overwritten.\n",
    "experiment_presets = \"Fig5_Red_Dotted\"\n",
    "\n",
    "## The demonstration numbers. Recommended: 0, 1, 2, 4, 8, 12.\n",
    "k = 4 \n",
    "\n",
    "## The used dataset index from the StaICC library. Alternative: 0, 1, 2, 3, 4, 5.\n",
    "dataset_index = 2 \n",
    "\n",
    "## Force the ICL_model to reload, even the ICL_model is already in the variables. \n",
    "## Recommended: False.\n",
    "model_forced_reload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load the experiment presets.\n",
    "\n",
    "if experiment_presets is not None:\n",
    "    print(\"Using the experiment presets. Overwriting the parameters.\")\n",
    "    trained_token_type, predicted_token_type, corr_label, no_context_baseline = EXPERIMENT_PRESETS[experiment_presets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57a11d-912c-485e-a9a6-8f3c0ebb0634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load the data and build the test inputs.\n",
    "\n",
    "bench = StaICC.Normal(k)\n",
    "prompts, queries = load_model_and_data.load_data_from_StaICC_experimentor(bench[dataset_index], predicted_token_type, corr_label)\n",
    "if no_context_baseline:\n",
    "    prompts = bench[dataset_index].get_label_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a4f0d50-8688-4e0e-94e2-81a9ae6d10d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Load the model.\n",
    "\n",
    "vars_dict = vars() if \"ICL_model\" in vars() else locals()\n",
    "if \"ICL_model\" not in vars_dict or model_forced_reload:\n",
    "    ICL_model, ICL_tknz = load_model_and_data.load_ICL_model(ICL_model_name, huggingface_token = huggingface_token, quantized = quantized)\n",
    "    loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ed78f",
   "metadata": {},
   "source": [
    "**Part II: Run the Experiment**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 6 - 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8062b40-088c-4b80-940b-60ffe26c1ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Get the ICL hidden states that should be used for the prediction of the centroid classifier.\n",
    "\n",
    "ICL_hidden_states = inference.ICL_inference_to_hidden_states(ICL_model, ICL_tknz, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86162a0-b915-452f-8cff-46549cbc3e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7: Train the centroid classifier, one for each layer of the ICL hidden states.\n",
    "\n",
    "inferencer = layered_hidden_calibration.layered_hidden_calibration(\n",
    "    bench[dataset_index].get_label_space(), \n",
    "    len(ICL_hidden_states), \n",
    "    trained_token_type, \n",
    "    corr_label if trained_token_type == \"label_words\" else True\n",
    ")\n",
    "inferencer.train(\n",
    "    bench[dataset_index].prompt_former, \n",
    "    functools.partial(inference.ICL_inference_to_hidden_states_transposed, model = ICL_model, tokenizer = ICL_tknz),\n",
    "    calibration_set = bench[dataset_index].calibration_set(),\n",
    "    calibration_number = 256,\n",
    "    k = k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2d81023-9088-4e25-acba-e312e3ce97a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 8: Decode the ICL hidden states by the centroid classifier, and evalate the performance by StaICC.\n",
    "\n",
    "inf_res = inferencer.batched_layered_inference(ICL_hidden_states)\n",
    "acc_in_layer = []\n",
    "for i in range(len(ICL_hidden_states)):\n",
    "    acc_in_layer.append(bench[dataset_index](forward_inference = None, input_prediction = inf_res[i] + inf_res[i])[0]['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b06d7",
   "metadata": {},
   "source": [
    "**Part III: Plot and Save the Result**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 9 and 10. You can define your own file name and dictionary to save the result in Cell 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd965f9c-0872-4cd9-95ab-7cb34a00b905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 9: Data preview.\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.xlabel(\"Transformer Block Number\", fontsize = 12)\n",
    "plt.ylabel(\"Accuracy\", fontsize = 12)\n",
    "plt.title(\"Centroid Classifier Acc. on dataset \" + str(dataset_index) + \"\\n model: \" + ICL_model_name + \"\\ntrained on: \" + trained_token_type + \", k: \" + str(k) , fontsize = 12)\n",
    "plt.plot(acc_in_layer, \n",
    "         color = ((\"green\" if corr_label else \"red\") if not no_context_baseline else \"gray\"), \n",
    "         label = (\"predicted on: \" + predicted_token_type) if not no_context_baseline else \"Correct Label w/o Context\",\n",
    "         linestyle = \"--\" if trained_token_type == \"label_words\" else \"-\",\n",
    "        )\n",
    "plt.axhline(1/len(bench[dataset_index].get_label_space()), color = \"black\", linestyle = \"--\", linewidth = 1, label = \"Random Baseline\")\n",
    "plt.legend(loc = 4, prop={'size': 9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a104cefe-d727-4cfb-ba2b-f9d0970b197a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 10: Save the result.\n",
    "# Result file organization:\n",
    "# list[layer_index] = the accuracy predicted in layer `layer_index`.\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_file_name = \"data/\" + ICL_model_name.replace('/', '_')+ \",\" + predicted_token_type + \",HiddenC,\" + trained_token_type  + ',' + predicted_token_type + ',' + (\"corr,\" if corr_label else \"wrong,\") + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "with open(data_file_name, 'wb') as f:\n",
    "    pickle.dump(acc_in_layer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
