{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5e6978",
   "metadata": {},
   "source": [
    "**Official implementation of paper \"Revisiting In-context Learning Inference Circuit in Large Language Models\" (ICLR 2025)**:\n",
    "\n",
    "### Forerunner Token Head Counting\n",
    "\n",
    "This experiment is to count the number of forerunner token heads in each layer, and also the maximum copy magnitude, for the Fig. 5 (Middle) and Fig. 22 - 25.\n",
    "\n",
    "Author: Hakaze Cho, yfzhao@jaist.ac.jp, 2024/09\n",
    "\n",
    "Organized, commented, and modified by: Hakaze Cho, 2025/01/28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661da871",
   "metadata": {},
   "source": [
    "**Part I: Import, Define, and Load Everything**\n",
    "\n",
    "What you should do:\n",
    "1. [Cell 1] Change to the path from your working directory to the directory containing the README.md file.\n",
    "2. [Cell 2] Define your experiment parameters.\n",
    "3. Run the Cell 1 - 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9657c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and change the working directory.\n",
    "\n",
    "## Change the working directory\n",
    "import os\n",
    "try:\n",
    "    # Change to the path from your working directory to the directory containing the README.md file.\n",
    "    os.chdir(\"ICL_Inference_Dynamics_Released\") \n",
    "except:\n",
    "    print(\"Already in the correct directory or the directory does not exist.\")\n",
    "\n",
    "## Import libraries\n",
    "from util import load_model_and_data, inference\n",
    "import StaICC\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "## Some definations for the plots.\n",
    "plt.style.use('default')\n",
    "plt.rc('font',family='Cambria Math')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Cambria Math'] + plt.rcParams['font.serif']\n",
    "\n",
    "## Calculate the attention threshold from the prompt.\n",
    "def get_thereshold_from_prompt(tokenizer, prompt, induction_threthold_times):\n",
    "    tkized = tokenizer(prompt)['input_ids']\n",
    "    return induction_threthold_times / len(tkized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80aa5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model and huggingfacetoken configurations\n",
    "\n",
    "## The huggingface model name to be tested as the LM for ICL. \n",
    "## Recommended: \"meta-llama/Meta-Llama-3-8B\", \"tiiuae/falcon-7b\", \"meta-llama/Meta-Llama-3-70B\", \"tiiuae/falcon-40b\"\n",
    "ICL_model_name = \"meta-llama/Meta-Llama-3-70B\"\n",
    "\n",
    "## Whether to use the quantized version of the model. \n",
    "## Recommended: Keep it default.\n",
    "quantized = False if ICL_model_name in [\"meta-llama/Meta-Llama-3-8B\", \"EleutherAI/pythia-6.9b\", \"tiiuae/falcon-7b\"] else True\n",
    "\n",
    "## The huggingface token to access the model. If you use the Llama model, you need to set this.\n",
    "huggingface_token = \"your token here\"\n",
    "\n",
    "## Use CPU instead of GPU to process this experiment. \n",
    "## Recommended: Only to use when you have the intermediate results in path `experiment_matrial` (shown below)\n",
    "cpu_process = False\n",
    "\n",
    "# Experiment parameters\n",
    "\n",
    "## The demonstration numbers. Recommended: 0, 1, 2, 4, 8, 12.\n",
    "k = 4 \n",
    "\n",
    "## The used dataset index from the StaICC library. Alternative: 0, 1, 2, 3, 4, 5. See the README.md for more information.\n",
    "dataset_index = 2 \n",
    "\n",
    "## Whether the **last** label in the prompt is (True) correct label or (False) wrong label.\n",
    "corr_label = True\n",
    "\n",
    "## Force the ICL_model to reload, even the ICL_model is already in the variables. \n",
    "## Recommended: False.\n",
    "model_forced_reload = False\n",
    "\n",
    "## Force the experiment to be redone, even the intermediate results are already in the path `experiment_material`.\n",
    "## Recommended: False.\n",
    "experiment_forced_redo = False\n",
    "\n",
    "## Define the const in the threshold to be judged as a forerunner token head. (e.g., \"5\" in 5/n_t)\n",
    "forerunner_token_head_threshold_times = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57a11d-912c-485e-a9a6-8f3c0ebb0634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Load the data and build the test inputs.\n",
    "\n",
    "bench = StaICC.Normal(k)\n",
    "prompts, queries = load_model_and_data.load_data_from_StaICC_experimentor(bench[dataset_index], \"label_words\", corr_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f0d50-8688-4e0e-94e2-81a9ae6d10d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load the model. If the intermediate results in path `experiment_matrial` (shown below) has been detected, automatically skip.\n",
    "\n",
    "data_file_name = \"experiment_matrial/\" + ICL_model_name.replace('/', '_')+ \",\" + \",copy_Hidd_att\" + ',' + (\"\" if corr_label else \"wrong,\") + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "if not os.path.exists(data_file_name) or experiment_forced_redo:\n",
    "    vars_dict = vars() if \"ICL_model\" in vars() else locals()\n",
    "    if \"ICL_model\" not in vars_dict or model_forced_reload:\n",
    "        ICL_model, ICL_tknz = load_model_and_data.load_ICL_model(ICL_model_name, huggingface_token = huggingface_token, quantized = quantized, device = \"cpu\" if cpu_process else \"cuda\")\n",
    "        loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49743007",
   "metadata": {},
   "source": [
    "**Part II: Run the Experiment**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 5 - 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8062b40-088c-4b80-940b-60ffe26c1ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Inference the hidden states and save the intermediate results. If the intermediate results in path `experiment_matrial` (shown below) has been detected, automatically load the results.\n",
    "\n",
    "data_file_name = \"experiment_matrial/\" + ICL_model_name.replace('/', '_')+ \",\" + \",copy_Hidd_att\" + ',' + (\"\" if corr_label else \"wrong,\") + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "if os.path.exists(data_file_name) and not experiment_forced_redo:\n",
    "    with open(data_file_name, 'rb') as f:\n",
    "        ICL_hidden_states = pickle.load(f)\n",
    "        print(\"Intermediate results loaded\")\n",
    "else:\n",
    "    ICL_hidden_states = inference.step2_get_fl_feature_and_lastftol_attention(ICL_model, ICL_tknz, prompts)\n",
    "    with open(data_file_name, 'wb') as f:\n",
    "        pickle.dump(ICL_hidden_states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541312e7-4b7c-4ffd-94b3-82545d5758c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Calculate the counted times for each attention head.\n",
    "\n",
    "counted_times_for_each_head = [[0 for i in range(len(inference.get_copy_magnitude_for_single_layer(ICL_hidden_states[1], 0, 0)))] for i in range(len(ICL_hidden_states[1][0]))]\n",
    "for layers in range(len(ICL_hidden_states[1][0])):\n",
    "    temp = []\n",
    "    head_count = []\n",
    "    for sample in range(len(ICL_hidden_states[1])):\n",
    "        thre = get_thereshold_from_prompt(ICL_tknz, prompts[sample], forerunner_token_head_threshold_times)\n",
    "        magnitudes = inference.get_copy_magnitude_for_single_layer(ICL_hidden_states[1], sample, layers)\n",
    "        for headindex in range(len(magnitudes)):\n",
    "            if ICL_hidden_states[1][sample][layers][headindex][0] > thre:\n",
    "                counted_times_for_each_head[layers][headindex] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802a8776-0eba-470c-b154-43bfd22eea71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7: Calculate the counted times summary for each layer.\n",
    "\n",
    "mean_max_magnitude = []\n",
    "mean_head_count = []\n",
    "\n",
    "for layers in range(len(ICL_hidden_states[1][0])):\n",
    "    temp = []\n",
    "    head_count = []\n",
    "    for sample in range(len(ICL_hidden_states[1])):\n",
    "        thre = get_thereshold_from_prompt(ICL_tknz, prompts[sample], forerunner_token_head_threshold_times)\n",
    "        magnitudes = inference.get_copy_magnitude_for_single_layer(ICL_hidden_states[1], sample, layers)\n",
    "        temp.append(max(magnitudes))\n",
    "        count = 0\n",
    "        for temp_res in magnitudes:\n",
    "            if temp_res > thre:\n",
    "                count += 1\n",
    "        head_count.append(count)\n",
    "    mean_max_magnitude.append(np.mean(temp))\n",
    "    mean_head_count.append(np.mean(head_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1950b",
   "metadata": {},
   "source": [
    "**Part III: Plot and Save the Result**\n",
    "\n",
    "What you should do:\n",
    "\n",
    "1. Run the Cell 8 - 11. You can define your own file name and dictionary to save the result in Cell 9 and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cd622-8217-445c-a3c6-5578bdd9d46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 8: Plot the counted times for each attention head in a heatmap.\n",
    "\n",
    "r = plt.imshow(counted_times_for_each_head, cmap = 'Blues' if corr_label else \"Purples\", vmin = 128, vmax = 512)\n",
    "plt.yticks(range(0, len(counted_times_for_each_head))[::10], range(1, 1 + len(counted_times_for_each_head))[::10])\n",
    "plt.colorbar(r, shrink=0.5)\n",
    "plt.xlabel('Head #', fontsize = 12)\n",
    "plt.ylabel('Transformer Block', fontsize = 12) \n",
    "plt.title(\"Forerunner Token Head to \" + (\"Correct Label\" if corr_label else \"Wrong Label\") + \"\\n Dataset \" + str(dataset_index + 1) + \" with k = \" + str(k) + \"\\n model: \" + ICL_model_name, fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c69a8afb-c66c-47f7-8571-1a8d1e52b7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 9: Save the heat map data.\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_file_name = \"data/\" + ICL_model_name.replace('/', '_')+ \",\" + \"copy_magnitude,headstat\"  + ',' + (\"\" if corr_label else \"wrong,\") + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "with open(data_file_name, 'wb') as f:\n",
    "    pickle.dump(counted_times_for_each_head, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Plot the figure similar to the Fig. 5 (Middle)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(range(1,len(mean_head_count) + 1), mean_head_count, \n",
    "        label = \"Correct Label\" if corr_label else \"Wrong Label\",\n",
    "        color = \"#023858\" if corr_label else \"#ff7f0e\"\n",
    ")\n",
    "\n",
    "ax.set_xlim(-1, len(mean_head_count) + 1)\n",
    "ax.set_xlabel(\"Transformer Block Number\", fontsize = 12)\n",
    "ax.set_ylabel(\"Forerunner Token Head #\", fontsize = 12)\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "xrange = ax.get_xticks()\n",
    "xrange[1] = 1\n",
    "plt.xticks(xrange[1:-1])\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"Maximum Copy Magnitude\", fontsize = 12)\n",
    "ax2.fill_between(range(1,len(mean_max_magnitude) + 1), mean_max_magnitude, color = \"#023858\" if corr_label else \"#ff7f0e\", alpha = 0.2)\n",
    "ax2.set_ylim((0, 1.1))\n",
    "\n",
    "ax.set_zorder(3)\n",
    "ax2.set_zorder(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469ecb9d-6bc5-48f4-8848-4d4c3248020b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 11: Save the figure data.\n",
    "# Result file organization:\n",
    "# (mean_max_magnitude: list[layer_index] = max forerunner token copy attention, mean_head_count: list[layer_index] = forerunner token head count)\n",
    "\n",
    "data_file_name = \"data/\" + ICL_model_name.replace('/', '_') + \",copy_magnitude\" + ',' + (\"\" if corr_label else \"wrong,\") + str(k) + ',' + str(dataset_index + 1) + \".pickle\"\n",
    "with open(data_file_name, 'wb') as f:\n",
    "    pickle.dump([mean_max_magnitude, mean_head_count], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
